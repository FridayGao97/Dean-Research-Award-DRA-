{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ECE 449 - Intelligent Systems Engineering<br><br>\n",
    "Lab 3: Neural Networks, Perceptrons and Hyperparameters</center></h1>\n",
    "<hr>\n",
    "<b>Lab date:</b> <i>Thursday, October 17, 2019 -- 2:00 - 4:50 PM</i>\n",
    "<br>\n",
    "<b>Room:</b> <i>ETLC E5-013<i/>\n",
    "<br>\n",
    "<b>Lab report due:</b> <i>Wednesday, October 30, 2019 -- 3:50 PM</i>\n",
    "<hr>\n",
    "\n",
    "<h2> 1. Objective:  </h2>\n",
    "\n",
    "The objective of this lab is to gain familiarity with the concepts of linear models and to gain a feeling for how changing hyperparameters affects the performance of the model. The exercises in the lab will help bring to light the weaknesses and strengths of linear models and how to work with them.\n",
    "\n",
    "<h2> 2. Expectation: </h2> \n",
    "\n",
    "Complete the pre-lab, and hand it in before the lab starts.  A formal lab report is required for this lab, which will be the completed version of this notebook.  There is a marking guide at the end of the lab manual.  If figures are required, label all the axies and provide a legend when appropriate.  An abstract, introduction, and conclusion are required as well, for which cells are provided at the end of the notebook.  The abstract should be a brief description of the topic, the introduction a description of the goals of the lab, and the conclusion a summary of what you learned, what you found difficult, and your own ideas and observations.\n",
    "\n",
    "<h2> 3. Pre lab: </h2>\n",
    "\n",
    "1. Read through the code. What kind of models will be used in this lab?\n",
    "2. Explain why the differentiability of an activation function plays an important role in the learning of these neural networks.  Why might the linear activation function be a poor choice in some cases?\n",
    "\n",
    "<h2> 4. Introduction: </h2>\n",
    "\n",
    "During this lab, you will be performing a mix of 2 common machine learning tasks: regression and classification. Before defining these tasks mathematically, it is important to understand the core process behind the two tasks. Regression is defined as reasoning backwards. In the context of machine learning, regression is about predicting the future based on the past. Classification is defined as the act of arranging things based on their properties. These definitions give insight into how these problems are broken down.\n",
    "\n",
    "Suppose you, a human being, want to make a prediction. What are the steps that you take:\n",
    "You first collect data on the subject that you want to predict. Then you weigh the relevance of each piece of information that you get, attributing varying levels of importance to each piece of data. Once you have enough relevant data, you become certain of an outcome. Finally, you act on that certainty. This pipeline is shown in the figure below:\n",
    "\n",
    "\n",
    "![alt text](Prediction_flowchart_v3.png \"Prediction Pipeline\")\n",
    "\n",
    "Now the classification task, one usually begins this with a topic that they want to classify. This is usually accompanied by a list of candidate categories, one of which is the correct category for the topic in question. Since classification relies on properties of the topic, the next step is to list the notable features that may help in the discerning the correct category. Similarly to the prediction, the relevance of each piece of information is then weighed and a decision is made when you have enough data. Once this is done, the guess is compared to reality in order to judge if the classification was correct. This pipeline is shown in the figure below:    \n",
    "\n",
    "\n",
    "![alt text](Prediction_flowchart_v3.png \"Classification Pipeline\")\n",
    "\n",
    "\n",
    "\n",
    "The mathematical model that we will use in this lab to describe such behaviors are called linear models. The simplest linear model is the perceptron.\n",
    "\n",
    "A *perceptron* is a simple type of neural network that uses *supervised learning*, where the expected values, or *targets*, are provided to the network in addition to the inputs.   The network operates by first calculating the weighted sum of its inputs (and bias).  These weights are typically randomly assigned.  Then, the sum is processed with an *activation function* that \"squashes\" the summed total to a smaller range, such as (0, 1).  \n",
    "\n",
    "The perceptron's way of reasoning is formulated in the same way as a human's. It takes in input data in the form of the x vector. It then weighs the relevance of each input using the mathematical operation of multiplication. Following this, the total sum of all weighted inputs is passed through an activation function, analogous to the moment that you have enough  data to confirm an outcome. Then they output a value, y, that is effectively the action that you take based on your prediction.\n",
    "\n",
    "The math behind the perceptron's operations is described by the following formulae:\n",
    "<br>\n",
    "$$\n",
    "tot = \\sum_{t=1}^{n} {x_i w_i} + \\theta =  \\sum_{t=0}^{n} {x_i w_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o = f_{act}(tot)\n",
    "$$\n",
    "<br> \n",
    "Training a perceptron involves calculating the error by taking the difference between the targets and the actual outputs.  This allows it to determine how to update its weights such that a closer output value to the target is obtained.\n",
    "<br>\n",
    "Perceptrons are commonly employed to solve two-class classification problems where the classes are linearly separable.  However, the applications for this are evidently very limited.  Therefore, a more practical extension of the perceptron is the *multi-layer perceptron*, which adds extra hidden layer(s) between the inputs and outputs to yield more flexibility in what the MLP can classify.\n",
    "<br>\n",
    "The most common learning algorithm used is *backpropagation (BP)*.  It employs gradient descent in an attempt to minimize the squared error between the network outputs and the targets.\n",
    "<br>\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{k=1}^{n}\\sum_{i=1}^{q}[t_{i}(k) - o_{i}(k)]^2\n",
    "$$\n",
    "<br>\n",
    "This error value is propagated backwards through the network, and small changes are made to the weights in each layer.  Using a gradient descent approach, the weights in the network are updated as follows:\n",
    "<br>\n",
    "$$\n",
    "\\Delta w^{(l)} = - \\eta \\nabla w^{(l)} = - \\eta \\frac{\\partial E(k)}{\\partial w^{(l)}}\n",
    "$$\n",
    "<br>\n",
    "where $\\eta > 0$ is the *learning rate*.  The network is trained using the same data, multiple times in *epochs*.  Typically, this continues until the network has reached a convergence point that is defined by the user through a *tolerance* value.  For the case of this lab, the tolerance value is ignored and training will continue until the specified number of epochs is reached.  More details of backpropagation can be found in the lecture notes.\n",
    "\n",
    "\n",
    "Neural networks have two types of parameters that affect the performance of the network, parameters and hyperparameters. Parameters have to do with the characteristics that the model learns during the training process. Hyperparameters are values that are set before training begins. The parameters of linear models are the weights. The hyperparameters include: \n",
    "- Learning algorithm \n",
    "- Loss function \n",
    "- Learning rate \n",
    "- Activation function\n",
    "\n",
    "\n",
    "Hyperparameter selection is very important in the field of AI in general. The performance of the learning systems that are deployed relies hevily on the selection of hyperparameters and some advances in the field have even been soley due to changes in hyperparametes. More on hyperparameters can be found in the lecture notes and in the literature.\n",
    "\n",
    "<br>\n",
    "<h2> 5. Experimental Procedure:</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Perceptrons and their limitations \n",
    "\n",
    "The objective of this exercise is to show how adding depth to the network makes it learn better. This exercise will involve running the following cells and examining the data. This exercise will showcase the classification task and it will be performed on the Iris dataset. Also, ensure that all files within \"Lab 3 Resources\" is placed in the same directory as this Jupyter notebook.\n",
    "<br>\n",
    "Run the following cell to import all the required libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np                               # General math operations\n",
    "import scipy.io as sio                           # Loads .mat variables\n",
    "import matplotlib.pyplot as plt                  # Data visualization\n",
    "from sklearn.linear_model import Perceptron      # Perceptron toolbox\n",
    "from sklearn.neural_network import MLPRegressor  # MLP toolbox\n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model                # Linear models\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset: This dataset contains data points on three different species of Iris, a type of flower. The dataset has 50 entries for each of the species and has 4 different features: \n",
    "<ol>\n",
    "<li>Sepal Length</li>\n",
    "<li>Sepal Width</li>\n",
    "<li>Petal Length</li> \n",
    "<li>Petal Width</li>\n",
    "</ol>\n",
    "\n",
    "This dataset has one obvious class that is separate from a cluster of the other two classes, making it a typical exercise in classification for machine learning. The next cell loads the dataset into 2 variables, one for the features and one for the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "iris = datasets.load_iris()\n",
    "Y = iris.target\n",
    "X = iris.data\n",
    "\n",
    "# set up the pandas dataframes \n",
    "X_df = pd.DataFrame(X, columns = ['Sepal length','Sepal width', 'Petal length', 'Petal width'] )\n",
    "Y_df = pd.DataFrame(Y, columns = ['Iris class'])\n",
    "\n",
    "# this code changes the class labels from numerical values to strings\n",
    "Y_df = Y_df.replace({\n",
    "0:'Setosa',\n",
    "1:'Virginica',\n",
    "2:'Versicolor'\n",
    "})\n",
    "\n",
    "#Joins the two dataframes into a single data frame for ease of use\n",
    "Z_df = X_df.join(Y_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data is a important tool for data exploration. Visualizing the data will allow you to intuitively understand obvious relationships that are present in the data, even before you begin to analyse it. The next cell will plot all of the features against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the data using seaborn \n",
    "sns.set(style='dark', palette= 'deep')\n",
    "pair = sns.pairplot(Z_df, hue = 'Iris class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of plot is called a pairplot. It plots each feature against all other features including itself; this is done for all four features. This results in 2 different types of plots being present in the plot, scatter and histogram. \n",
    "\n",
    "The following cell will train a perceptron on the features and labels and display the result on the test set in a pairplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 6\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X_df, Y_df, test_size =0.3,\\\n",
    "                                               random_state=RANDOM_SEED)\n",
    "#plot the testing data \n",
    "test_df = xTest.join(yTest)\n",
    "# print(test_df.head)\n",
    "# perceptron training\n",
    "percep = Perceptron(max_iter = 1000)\n",
    "percep.fit(xTrain, yTrain)\n",
    "prediction = percep.predict(xTest)\n",
    "\n",
    "# print(prediction)\n",
    "# display the classifiers performance  \n",
    "prediction_df = pd.DataFrame(prediction, columns=['Predicted Iris class'], index = test_df.index)\n",
    "# print(prediction_df.head)\n",
    "\n",
    "prediction_df_index_df = prediction_df.join(xTest)\n",
    "# print(prediction_df_index_df.head)\n",
    "\n",
    "pair = sns.pairplot(prediction_df_index_df, hue = 'Predicted Iris class')\n",
    "#pair_test = sns.pairplot(test_df, hue ='Iris class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_test = sns.pairplot(test_df, hue ='Iris class') #test data from the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "**Comment on the performance of the perceptron, how well does it handle the task?**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will retrain the perceptron but with different parameters. This MLP consists of 2 hidden layers: one with 8 neurons and a second one with 3 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the layers, retrain the mlp \n",
    "cls = MLPClassifier(solver = 'sgd' ,activation = 'relu' ,  \\\n",
    "                    hidden_layer_sizes = (8,3,), max_iter = 100000)\n",
    "\n",
    "for i in range(0,5):\n",
    "    cls.fit(xTrain, yTrain)\n",
    "\n",
    "mlp_z = cls.predict(xTest)\n",
    "\n",
    "mlp_z.reshape(-1,1)\n",
    "\n",
    "\n",
    "cls_df = pd.DataFrame(mlp_z, columns = [\"Mlp prediction\"], index=xTest.index)\n",
    "\n",
    "# cls_df_index = cls_df.join(Test_index_df).set_index('Test index')\n",
    "# cls_df_index.index.name = None \n",
    "\n",
    "# Join with the test_index frame \n",
    "cls_prediction_df = cls_df.join(xTest)\n",
    "# Display the MLP classifier\n",
    "cls_pairplot = sns.pairplot(cls_prediction_df, hue = 'Mlp prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Answer the following questions:\n",
    "\n",
    "- **How does the Mlp compare to the perceptron in the classification task?** \n",
    "- **Did it do well**\n",
    "- **Was it able to classify the three classes?** \n",
    "- **What happens when you run it again?**\n",
    "- **Can you offer a explanation for what happened?**\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Fill the box below with your answer:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Getting your hands dirty with regression\n",
    "\n",
    "**NOTE: The code in this exercise is computationally intensive and may require up to 5 minutes to finish running.**\n",
    "\n",
    "In order to improve the energy management of monitoring stations, a team of climatologists would like to implement a more robust strategy of predicting the solar energy available for the next day, based on the current day's atmospheric pressure.  They plan to test this with a station situated in Moxee, and are designing a multi-layer perceptron that will be trained with the previous year's worth of Moxee data.  They have access to the following values:\n",
    "<ul>\n",
    "    <li><b>Inputs:</b> Pressure values for each hour, along with the absolute differences between them</li>\n",
    "    <li><b>Targets:</b> Recorded solar energy for the day after</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "The individual who was in charge of this project before had created a traditional machine learning approach to predict the solar energy availiabilty of the next day. The individual recently retired and you have been brought on to the team to try to implement a more accurate system. You find some code that was left over that uses a MLP. The MLP is initially formed using one hidden layer of 50 neurons, a logistic sigmoid activation function, and a total of 500 iterations.  Once it is trained, the MLP is used to predict the results of both the training cases and new test cases.  As a measure of accuracy, the root mean square error (RMSE) is displayed after inputting data to the MLP.\n",
    "\n",
    "First, read through the code to understand what results it produces, and then run the script. \n",
    "\n",
    "### Question 1:\n",
    "<ol>\n",
    "    Your objective is to play with the parameters of the regressor to see if you can beat the decision tree. There are parameters that you can change to try to beat it. You can change:\n",
    "<ul>\n",
    "    <li>Size of the Hidden Layers: <b>between 1 and 50</b>  </li>\n",
    "    <li> Activation Function:</li>\n",
    "    <ul><li><b>Identity</b> </li>\n",
    "        <li><b>Logistic</b> </li>\n",
    "        <li><b>tanh</b> </li>\n",
    "        <li><b>relu</b> </li>\n",
    "        </ul>\n",
    "    <li>Number of Iterations, to different values (both lower and higher): <b>Between 1 and 1000</b></li>\n",
    "\n",
    "</ul>\n",
    "<br>\n",
    "Comment on how this affects the results. Include plots of your final results (using any one of your values for the parameters). Describe some of the tradeoffs associated with both lowering and raising the number of iterations.</li>\n",
    "\n",
    "\n",
    "In order to determine the accuracy of the methods, you will be using RMSE \n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{\\sum{(Approximated - observed)}}{n}}\n",
    "$$ \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain training data\n",
    "moxeeData = sio.loadmat('moxeetrainingdata.mat')    # Load variables from the Moxee dataset\n",
    "trainingInputs = moxeeData['pressureData']          # Pressure values and differences for every hour in a year\n",
    "trainingTargets = moxeeData['dataEstimate']         # Estimate of incoming solar energy based on observed data\n",
    "\n",
    "# Preprocess the training inputs and targets\n",
    "iScaler = preprocessing.StandardScaler()    # Scaler that removes the mean and scales to unit variance\n",
    "scaledTrainingInputs = iScaler.fit_transform(trainingInputs)   # Fit and scale the training inputs\n",
    "\n",
    "tScaler = preprocessing.StandardScaler()\n",
    "scaledTrainingTargets = tScaler.fit_transform(trainingTargets)\n",
    "\n",
    "# Create the multilayer perceptron.\n",
    "# This is where you will be modifying the regressor to try to beat the decision tree\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (1,),     # One hidden layer with 50 neurons\n",
    "    activation = 'logistic',        # Logistic sigmoid activation function\n",
    "    solver = 'sgd',                 # Gradient descent\n",
    "    learning_rate_init = 0.01 ,# Initial learning rate\n",
    "    )\n",
    "# \n",
    "############################################################### Create the decision tree:\n",
    "dt_reg = DecisionTreeRegressor(criterion='mse', max_depth = 10) \n",
    "dt_reg.fit(scaledTrainingInputs, scaledTrainingTargets)\n",
    "\n",
    "\n",
    "### MODIFY THE VALUE BELOW ###\n",
    "noIterations = 98  # Number of iterations (epochs) for which the MLP trains\n",
    "### MODIFY THE VALUE ABOVE ###\n",
    "\n",
    "trainingError = np.zeros(noIterations)  # Initialize array to hold training error values\n",
    "\n",
    "# Train the MLP for the specified number of iterations\n",
    "for i in range(noIterations):\n",
    "    mlp.partial_fit(scaledTrainingInputs, np.ravel(scaledTrainingTargets))  # Partial fit is used to obtain the output values after each epoch\n",
    "    currentOutputs = mlp.predict(scaledTrainingInputs)  # Obtain the outputs for the current MLP using the training inputs\n",
    "    trainingError[i] = np.sum((scaledTrainingTargets.T - currentOutputs) ** 2) / 2  # Keep track of the error throughout the number of epochs\n",
    "\n",
    "# Plot the error curve\n",
    "plt.figure(figsize=(10,6))\n",
    "ErrorHandle ,= plt.plot(range(noIterations), trainingError, label = 'Error 50HU',  linestyle = 'dotted')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Error of the MLP for Every Epoch')\n",
    "plt.legend(handles = [ErrorHandle])\n",
    "plt.show()\n",
    "\n",
    "# Obtain test data\n",
    "testdataset = sio.loadmat('moxeetestdata.mat')\n",
    "testInputs = testdataset['testInputs']\n",
    "testTargets = testdataset['testTargets']\n",
    "scaledTestInputs = iScaler.transform(testInputs)  # Scale the test inputs\n",
    "\n",
    "# Predict incoming solar energy from the training data and the test cases\n",
    "scaledTrainingOutputs = mlp.predict(scaledTrainingInputs)\n",
    "scaledTestOutputs = mlp.predict(scaledTestInputs)\n",
    "\n",
    "#################################################################### Predict using the bad guy: \n",
    "scaledTreeTrainingOutputs = dt_reg.predict(scaledTrainingInputs)\n",
    "scaledTreeTestOutputs = dt_reg.predict(scaledTestInputs)\n",
    "\n",
    "# Transform the outputs back to the original values\n",
    "trainingOutputs = tScaler.inverse_transform(scaledTrainingOutputs)\n",
    "testOutputs = tScaler.inverse_transform(scaledTestOutputs)\n",
    "## DT outputs \n",
    "treeTrainingOutputs = tScaler.inverse_transform(scaledTreeTrainingOutputs) # -- transform the tree back to real data \n",
    "treeTestingOutputs = tScaler.inverse_transform(scaledTreeTestOutputs)\n",
    "\n",
    "# Calculate and display training and test root mean square error (RMSE)\n",
    "trainingRMSE = np.sqrt(np.sum((trainingOutputs - trainingTargets[:, 0]) ** 2) / len(trainingOutputs)) / 1000000  # Divide by 1e6 for MJ/m^2\n",
    "testRMSE = np.sqrt(np.sum((testOutputs - testTargets[:, 0]) ** 2) / len(testOutputs)) / 1000000\n",
    "\n",
    "## need to add this for the decision tree \n",
    "trainingTreeRMSE = np.sqrt(np.sum((treeTrainingOutputs - trainingTargets[:, 0]) ** 2) / len(trainingOutputs)) / 1000000\n",
    "testTreeRMSE = np.sqrt(np.sum((treeTestingOutputs - testTargets[:, 0]) ** 2) / len(testOutputs)) / 1000000\n",
    "\n",
    "print(\"Training RMSE:\", trainingRMSE, \"MJ/m^2\")\n",
    "print(\"Test RMSE:\", testRMSE, \"MJ/m^2\")\n",
    "##################################################################### Print the tree RMSE:\n",
    "print(\"Decision Tree training RMSE:\", trainingTreeRMSE, 'MJ/m^2')\n",
    "print(\"Decision Tree Test RMSE:\", testTreeRMSE, 'MJ/m^2')\n",
    "day = np.array(range(1, len(testTargets) + 1))\n",
    "\n",
    "# Plot training targets vs. training outputs\n",
    "plt.figure(figsize=(10,6))\n",
    "trainingTargetHandle ,= plt.plot(day, trainingTargets / 1000000, label = 'Target values')\n",
    "trainingOutputHandle ,= plt.plot(day, trainingOutputs / 1000000, label = 'Outputs 50HU',  linestyle = 'dotted')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel(r'Incoming Solar Energy [$MJ / m^2$]')\n",
    "plt.title('Comparison of MLP Training Targets and Outputs')\n",
    "plt.legend(handles = [trainingTargetHandle, trainingOutputHandle])\n",
    "plt.show()\n",
    "\n",
    "# Plot test targets vs. test outputs -- student \n",
    "plt.figure(figsize=(10,6))\n",
    "testTargetHandle ,= plt.plot(day, testTargets / 1000000, label = 'Target values')\n",
    "testOutputHandle ,= plt.plot(day, testOutputs / 1000000, label = 'Outputs 50HU',  linestyle = 'dotted')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel(r'Incoming Solar Energy [$MJ / m^2$]')\n",
    "plt.title('Comparison of MLP Test Targets and Outputs')\n",
    "plt.legend(handles = [testTargetHandle, testOutputHandle])\n",
    "plt.show()\n",
    "\n",
    "###################################################################### Plot the tree regressor vs. test outputs\n",
    "plt.figure(figsize=(10,6))\n",
    "testTreeTargetHandle, = plt.plot(day, testTargets / 1000000, label = 'Target values')\n",
    "testTreeOutputHandle, = plt.plot(day, treeTestingOutputs / 1000000, label = 'Decision tree', linestyle = 'dotted')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel(r'Incoming Solar Energy [$MJ / m^2$]')\n",
    "plt.title('Comparison of Decision Tree Test Targets and Outputs')\n",
    "plt.legend(handles = [testTreeTargetHandle, testTreeOutputHandle])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "Fill the box below with your answer for question 1:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process but against this SVM:\n",
    "\n",
    "\n",
    "During a coffee break, you get talking to one of your friends from a different department. He mentioned that at one point there was an intern that was also tasked with predicting the solar energy and they tried a Support Vector machine. \n",
    "\n",
    "When you tell your superiors, they suggest that you try to beat this interns work as well since it seems to work better than the Decision tree that your predecessor left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "  Your objective again is to play with the parameters of the regressor to see if you can beat the Support Vector Machine. There are parameters that you can change to try to beat it. You can change:\n",
    "<ul>\n",
    "    <li>Size of the Hidden Layers: <b>between 1 and 50</b>  </li>\n",
    "    <li> Activation Function:</li>\n",
    "    <ul><li><b> Identity </b> </li>\n",
    "        <li><b>Logistic</b> </li>\n",
    "        <li><b>tanh</b> </li>\n",
    "        <li><b>relu</b> </li>\n",
    "        </ul>\n",
    "    <li> Number of Iterations, to different values (both lower and higher): <b>Between 1 and 1000</b></li>\n",
    "\n",
    "</ul>\n",
    "<br>\n",
    "Comment on how this affects the results. Include plots of your final results (using any one of your values for the parameters). Describe some of the tradeoffs associated with both lowering and raising the number of iterations.</li>\n",
    "\n",
    "\n",
    "In order to determine the accuracy of the methods, you will be using the RMSE again. \n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{\\sum{(Approximated - observed)}}{n}}\n",
    "$$ \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE \n",
    "from sklearn.svm import LinearSVR\n",
    "svm_clf = LinearSVR(C=0.6, loss='squared_epsilon_insensitive')\n",
    "svm_clf.fit(scaledTrainingInputs, np.ravel(scaledTrainingTargets)) \n",
    "\n",
    "# PREDICT the training outputs and the test outputs\n",
    "scaledTrainingOutputs = svm_clf.predict(scaledTrainingInputs)\n",
    "scaledTestOutputs = svm_clf.predict(scaledTestInputs)\n",
    "\n",
    "\n",
    "trainingOutputs = tScaler.inverse_transform(scaledTrainingOutputs)\n",
    "testOutputs = tScaler.inverse_transform(scaledTestOutputs)\n",
    "\n",
    " #Calculate and display training and test root mean square error (RMSE)\n",
    "trainingsvmRMSE = np.sqrt(np.sum((trainingOutputs - trainingTargets[:, 0]) ** 2) / len(trainingOutputs)) / 1000000  # Divide by 1e6 for MJ/m^2\n",
    "testsvmRMSE = np.sqrt(np.sum((testOutputs - testTargets[:, 0]) ** 2) / len(testOutputs)) / 1000000\n",
    "\n",
    "#### PLOTTING\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "day = np.array(range(1, len(testTargets) + 1))\n",
    "\n",
    "testTargetHandle, = plt.plot(day, testTargets / 1000000, label = 'Target Values')\n",
    "testsvmOutputHandle, = plt.plot(day, testOutputs / 1000000, label = 'SVM Prediction', linestyle = 'dotted')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel(r'Incoming Solar Energy [$MJ / m^2$]')\n",
    "plt.title('Comparison of Prediction Targets and SVM Predictions')\n",
    "plt.legend(handles = [testTargetHandle, testsvmOutputHandle])\n",
    "plt.show()\n",
    "\n",
    "print(\"Support Vector Machine RMSE values and Plots\")\n",
    "print(\"Training RMSE:\", trainingsvmRMSE, \"MJ/m^2\")\n",
    "print(\"Test RMSE:\", testsvmRMSE, \"MJ/m^2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Modify this neural network \n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes = (1,),     # One hidden layer with 50 neurons\n",
    "    activation = 'logistic',        # Logistic sigmoid activation function\n",
    "    solver = 'sgd',                 # Gradient descent\n",
    "    learning_rate_init = 0.01 ,# Initial learning rate\n",
    "    )\n",
    "# \n",
    "############################################################### Create the decision tree:\n",
    "dt_reg = DecisionTreeRegressor(criterion='mse', max_depth = 10) \n",
    "dt_reg.fit(scaledTrainingInputs, scaledTrainingTargets)\n",
    "\n",
    "\n",
    "### MODIFY THE VALUE BELOW ###\n",
    "noIterations = 98  # Number of iterations (epochs) for which the MLP trains\n",
    "### MODIFY THE VALUE ABOVE ###\n",
    "\n",
    "trainingError = np.zeros(noIterations)  # Initialize array to hold training error values\n",
    "\n",
    "# Train the MLP for the specified number of iterations\n",
    "for i in range(noIterations):\n",
    "    mlp.partial_fit(scaledTrainingInputs, np.ravel(scaledTrainingTargets))  # Partial fit is used to obtain the output values after each epoch\n",
    "    currentOutputs = mlp.predict(scaledTrainingInputs)  # Obtain the outputs for the current MLP using the training inputs\n",
    "    trainingError[i] = np.sum((scaledTrainingTargets.T - currentOutputs) ** 2) / 2  # Keep track of the error throughout the number of epochs\n",
    "    \n",
    "# Predict \n",
    "scaledTrainingOutputs = mlp.predict(scaledTrainingInputs)\n",
    "scaledTestOutputs = mlp.predict(scaledTestInputs)\n",
    "#Training output conversion    \n",
    "trainingOutputs = tScaler.inverse_transform(scaledTrainingOutputs)\n",
    "testOutputs = tScaler.inverse_transform(scaledTestOutputs)\n",
    "\n",
    "#RMSE calculation \n",
    "trainingRMSE = np.sqrt(np.sum((trainingOutputs - trainingTargets[:, 0]) ** 2) / len(trainingOutputs)) / 1000000  # Divide by 1e6 for MJ/m^2\n",
    "testRMSE = np.sqrt(np.sum((testOutputs - testTargets[:, 0]) ** 2) / len(testOutputs)) / 1000000\n",
    "    \n",
    "# Plot the error curve\n",
    "plt.figure(figsize=(10,6))\n",
    "ErrorHandle ,= plt.plot(range(noIterations), trainingError, label = 'Error 50HU',  linestyle = 'dotted')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Error of the MLP for Every Epoch')\n",
    "plt.legend(handles = [ErrorHandle])\n",
    "plt.show()\n",
    "\n",
    "print(\"MLP Training and test RMSE values:\")\n",
    "print(\"Training RMSE: \" , trainingRMSE)\n",
    "print(\"Test RMSE: \" , testRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "Fill the box below with your answer for question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Abstract</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h3>Lab 3 Marking Guide</h3>\n",
    "<hr>\n",
    "</center>\n",
    "\n",
    "\\begin{array}{@{}clcc@{}}\n",
    "\\textbf{Exercise} & \\textbf{Item}           & \\textbf{Total Marks} & \\textbf{Earned Marks} \\\\ \n",
    "\\hline\n",
    "                  & Pre-lab                 & 3                     &               \\\\ \n",
    "                  & Abstract                & 1                     &               \\\\ \n",
    "                  & Introduction            & 1                     &               \\\\\n",
    "                  & Conclusion              & 2                     &               \\\\\n",
    "1                 & Classification      \t& 20                    &               \\\\\n",
    "2                 & Regression              & 20                 \t                \\\\\n",
    "\\hline\n",
    "                  & \\textbf{TOTAL}          & 47                    &\n",
    "\\end{array}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
